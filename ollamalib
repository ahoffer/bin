#!/bin/bash
# Ollama shared library - source this in other scripts
# Usage: source ollamalib

OLLAMA_TIMEOUT="${OLLAMA_TIMEOUT:-120}"

# Stdin safety: all init code must use </dev/null on any command that
# might consume stdin, since consumers call ollama_capture_stdin after sourcing.
# Resolve Ollama host with fallback (hostname â†’ IP)
# Respects OLLAMA_HOST if already set in environment
OLLAMA_HOST="${OLLAMA_HOST:-}"
_resolve_ollama_host() {
    if [[ -n "$OLLAMA_HOST" ]]; then
        if ! curl -s --connect-timeout 2 "$OLLAMA_HOST/api/tags" </dev/null >/dev/null 2>&1; then
            echo "Error: Ollama not responding at $OLLAMA_HOST" >&2
            return 1
        fi
        return
    fi
    for host in "http://bigfish:11434" "http://192.168.0.70:11434"; do
        if curl -s --connect-timeout 2 "$host/api/tags" </dev/null >/dev/null 2>&1; then
            OLLAMA_HOST="$host"
            return
        fi
    done
    echo "Error: Cannot find Ollama server (tried bigfish:11434, 192.168.0.70:11434)" >&2
    echo "Set OLLAMA_HOST to specify a server, for example OLLAMA_HOST=http://host:11434" >&2
    return 1
}
_resolve_ollama_host
OLLAMA_USAGE_LOG="${OLLAMA_USAGE_LOG:-$HOME/.cache/ollama-usage.jsonl}"

# Read conversation ID from file if not set in environment
OLLAMA_CONVO_FILE="$HOME/.cache/ollama-conversation-id"
if [[ -z "${OLLAMA_CONVERSATION_ID:-}" && -f "$OLLAMA_CONVO_FILE" ]]; then
    OLLAMA_CONVERSATION_ID=$(cat "$OLLAMA_CONVO_FILE")
fi
OLLAMA_CONVERSATION_ID="${OLLAMA_CONVERSATION_ID:-}"

# Auto-derive tool name from the calling script
OLLAMA_TOOL_NAME="${OLLAMA_TOOL_NAME:-$(basename "${BASH_SOURCE[-1]:-$0}" | sed 's/^llm//')}"

# Models
MODEL_CHAT="${OLLAMA_MODEL_CHAT:-llama3.2:latest}"
MODEL_CODE="${OLLAMA_MODEL_CODE:-qwen2.5-coder:7b}"
MODEL_EMBED="${OLLAMA_MODEL_EMBED:-nomic-embed-text:latest}"
MODEL_VISION="${OLLAMA_MODEL_VISION:-llama3.2-vision:11b}"

# Log usage after each call
# Args: model, prompt_tokens, completion_tokens, tool_name
_log_usage() {
    local model="$1"
    local prompt_tokens="${2:-0}"
    local completion_tokens="${3:-0}"
    local tool="${4:-unknown}"
    local convo="${OLLAMA_CONVERSATION_ID:-}"
    mkdir -p "$(dirname "$OLLAMA_USAGE_LOG")"
    jq -nc --arg ts "$(date -Iseconds)" --arg model "$model" \
           --argjson prompt "$prompt_tokens" --argjson completion "$completion_tokens" \
           --arg tool "$tool" --arg convo "$convo" \
           '{ts:$ts,model:$model,prompt_tokens:$prompt,completion_tokens:$completion,tool:$tool,convo:$convo}' \
           >> "$OLLAMA_USAGE_LOG" 2>/dev/null || true
}

# Check if Ollama is reachable
ollama_ping() {
    curl -s --connect-timeout 2 "$OLLAMA_HOST/api/tags" </dev/null >/dev/null 2>&1
}

# Require Ollama to be reachable, exit if not
ollama_require() {
    if ! ollama_ping; then
        echo "Error: Cannot reach Ollama at $OLLAMA_HOST" >&2
        return 1
    fi
}

# Capture stdin into OLLAMA_STDIN if input is piped.
# Call after sourcing the library, not before.
# Optional byte limit as first argument, 0 means unlimited.
OLLAMA_STDIN=""
ollama_capture_stdin() {
    local byte_limit="${1:-0}"
    if [[ ! -t 0 ]]; then
        if [[ "$byte_limit" -gt 0 ]]; then
            OLLAMA_STDIN=$(head -c "$byte_limit")
        else
            OLLAMA_STDIN=$(cat)
        fi
    fi
}

# Read input from a file path, captured stdin, or fail with an error.
# Args: token_limit (default 4000), file_path (optional), error_noun (default "input")
ollama_read_input() {
    local token_limit="${1:-4000}"
    local file_path="${2:-}"
    local error_noun="${3:-input}"
    if [[ -n "$file_path" ]]; then
        if [[ ! -f "$file_path" ]]; then
            echo "Not a file: $file_path" >&2
            return 1
        fi
        truncate_input "$token_limit" < "$file_path"
    elif [[ -n "${OLLAMA_STDIN:-}" ]]; then
        printf '%s' "$OLLAMA_STDIN" | truncate_input "$token_limit"
    else
        echo "No $error_noun provided" >&2
        return 1
    fi
}

# Detect programming language from file extension
ollama_detect_lang() {
    case "$1" in
        *.py)         echo "python" ;;
        *.ts|*.tsx)   echo "typescript" ;;
        *.js|*.jsx)   echo "javascript" ;;
        *.java)       echo "java" ;;
        *.go)         echo "go" ;;
        *.rs)         echo "rust" ;;
        *.rb)         echo "ruby" ;;
        *.sh|*.bash)  echo "bash" ;;
        *)            echo "unknown" ;;
    esac
}

# Generate completion (streaming disabled for scripting)
# Args: model, prompt, [system_prompt], [tool_name]
ollama_generate() {
    local model="$1"
    local prompt="$2"
    local system="${3:-}"
    local tool="${4:-${OLLAMA_TOOL_NAME:-generate}}"

    local payload
    if [[ -n "$system" ]]; then
        payload=$(jq -n --arg model "$model" --arg prompt "$prompt" --arg system "$system" \
            '{model: $model, prompt: $prompt, system: $system, stream: false}')
    else
        payload=$(jq -n --arg model "$model" --arg prompt "$prompt" \
            '{model: $model, prompt: $prompt, stream: false}')
    fi

    local response
    if ! response=$(curl -s --max-time "$OLLAMA_TIMEOUT" \
        -H "Content-Type: application/json" \
        -d "$payload" \
        "$OLLAMA_HOST/api/generate" </dev/null 2>/dev/null); then
        echo "Error: Ollama request failed" >&2
        return 1
    fi

    local api_error
    api_error=$(echo "$response" | jq -r '.error // empty')
    if [[ -n "$api_error" ]]; then
        echo "Error: Ollama generate ($model): $api_error" >&2
        return 1
    fi

    # Log usage
    local prompt_tokens completion_tokens
    prompt_tokens=$(echo "$response" | jq -r '.prompt_eval_count // 0')
    completion_tokens=$(echo "$response" | jq -r '.eval_count // 0')
    _log_usage "$model" "$prompt_tokens" "$completion_tokens" "$tool"

    # Extract response text
    echo "$response" | jq -r '.response // empty'
}

# Chat completion with messages array
# Args: model, messages_json, [tool_name], [options_json]
ollama_chat() {
    local model="$1"
    local messages="$2"
    local tool="${3:-${OLLAMA_TOOL_NAME:-chat}}"
    local options_json="${4:-}"

    local payload
    if [[ -n "$options_json" ]]; then
        payload=$(jq -n --arg model "$model" --argjson messages "$messages" \
            --argjson opts "$options_json" \
            '{model: $model, messages: $messages, stream: false, options: $opts}')
    else
        payload=$(jq -n --arg model "$model" --argjson messages "$messages" \
            '{model: $model, messages: $messages, stream: false}')
    fi

    local response
    if ! response=$(curl -s --max-time "$OLLAMA_TIMEOUT" \
        -H "Content-Type: application/json" \
        -d "$payload" \
        "$OLLAMA_HOST/api/chat" </dev/null 2>/dev/null); then
        echo "Error: Ollama chat request failed" >&2
        return 1
    fi

    local api_error
    api_error=$(echo "$response" | jq -r '.error // empty')
    if [[ -n "$api_error" ]]; then
        echo "Error: Ollama chat ($model): $api_error" >&2
        return 1
    fi

    # Log usage
    local prompt_tokens completion_tokens
    prompt_tokens=$(echo "$response" | jq -r '.prompt_eval_count // 0')
    completion_tokens=$(echo "$response" | jq -r '.eval_count // 0')
    _log_usage "$model" "$prompt_tokens" "$completion_tokens" "$tool"

    echo "$response" | jq -r '.message.content // empty'
}

# Get embeddings
# Args: model, text
ollama_embed() {
    local model="$1"
    local text="$2"

    local payload
    payload=$(jq -n --arg model "$model" --arg input "$text" \
        '{model: $model, input: $input}')

    curl -s --max-time "$OLLAMA_TIMEOUT" \
        -H "Content-Type: application/json" \
        -d "$payload" \
        "$OLLAMA_HOST/api/embed" </dev/null 2>/dev/null | jq -c '.embeddings[0] // empty'
}

# Truncate input to approximate token limit (rough: 4 chars = 1 token)
truncate_input() {
    local max_tokens="${1:-4000}"
    local max_chars=$((max_tokens * 4))
    head -c "$max_chars"
}

# Convenience wrapper for simple system+user chat calls.
# Args: model, system_prompt, user_message, [tool_name], [options_json]
ollama_chat_simple() {
    local model="$1" system_prompt="$2" user_message="$3"
    local tool="${4:-${OLLAMA_TOOL_NAME:-chat}}"
    local options="${5:-}"
    local messages
    messages=$(jq -n --arg sys "$system_prompt" --arg usr "$user_message" \
        '[{role:"system",content:$sys},{role:"user",content:$usr}]')
    ollama_chat "$model" "$messages" "$tool" "$options"
}
